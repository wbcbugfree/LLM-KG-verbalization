{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b21acf48-994c-47ee-af93-8025f9b78719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import rdflib\n",
    "from rdflib import Graph, URIRef, Literal, Namespace, BNode\n",
    "from rdflib.namespace import SKOS, DCTERMS, DCMITYPE, RDF, RDFS, XSD, PROV, SDO, TIME, split_uri\n",
    "\n",
    "from openai import OpenAI\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import anthropic\n",
    "\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import PIL.Image\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d6e0afa-aed6-4b58-a057-55f34e1e317c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening config file, the config structure is:\n",
    "# \"openai_api_key\":\"......\"}\n",
    "\n",
    "config = open('config', 'r')\n",
    "config = json.load(config)\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = config['openai_api_key']\n",
    "os.environ['GEMINI_API_KEY'] = config['gemini_api_key']\n",
    "os.environ['XAI_API_KEY'] = config['xai_api_key']\n",
    "os.environ['NVIDIA_API_KEY'] = config['nvidia_api_key']\n",
    "os.environ['DEEPSEEK_API_KEY'] = config['deepseek_api_key']\n",
    "os.environ['ANTHROPIC_API_KEY'] = config['claude_api_key']\n",
    "os.environ['DASHSCOPE_API_KEY'] = config['dashscope_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "098ccaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully converted out_A_B_filtered.ttl to rdf_verbal_dataset.json\n"
     ]
    }
   ],
   "source": [
    "def convert_ttl_to_json(input_file_path, output_file_path):\n",
    "    \"\"\"\n",
    "    Converts a TTL-like text file to a JSON file with a specific structure.\n",
    "\n",
    "    This function reads each line from the input file, treats it as a string\n",
    "    for the \"rdf_star\" field, and adds two additional empty fields.\n",
    "\n",
    "    Args:\n",
    "        input_file_path (str): The path to the input .ttl file.\n",
    "        output_file_path (str): The path where the output .json file will be saved.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    try:\n",
    "        with open(input_file_path, 'r', encoding='utf-8') as f_in:\n",
    "            for line in f_in:\n",
    "                # Strip leading/trailing whitespace, including newlines\n",
    "                stripped_line = line.strip()\n",
    "                if stripped_line:  # Ensure the line is not empty\n",
    "                    record = {\n",
    "                        \"rdf_star\": stripped_line,\n",
    "                        \"zero_text_qwen3:235b\": \"\",\n",
    "                        \"one_text_qwen3:235b\": \"\"\n",
    "                    }\n",
    "                    dataset.append(record)\n",
    "\n",
    "        # Create the final JSON structure\n",
    "        final_json = {\"dataset\": dataset}\n",
    "\n",
    "        # Write the JSON data to the output file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f_out:\n",
    "            json.dump(final_json, f_out, indent=2)\n",
    "\n",
    "        print(f\"Successfully converted {input_file_path} to {output_file_path}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# --- How to use the script ---\n",
    "\n",
    "# 1. Make sure this Python script is in the same directory as your input file.\n",
    "# 2. Change 'out_A_B_filtered.ttl' to the actual name of your input file if it's different.\n",
    "# 3. The output will be saved as 'output.json' in the same directory.\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Define the input and output file names\n",
    "    input_filename = 'out_A_B_filtered.ttl'\n",
    "    output_filename = 'rdf_verbal_dataset.json'\n",
    "\n",
    "    # Run the conversion\n",
    "    convert_ttl_to_json(input_filename, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a576c24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON dataset\n",
    "with open('rdf_verbal_dataset.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Access the dataset\n",
    "dataset = data['dataset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415ada89",
   "metadata": {},
   "source": [
    "### Convert text to RDF triples using LLMs: zero-shot prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c9adb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_0t = \"\"\"# Prompt for Knowledge Graph Verbalization\n",
    "\n",
    "**Objective:** Your task is to convert a set of RDF triples, representing a small knowledge graph, into a coherent, natural-language paragraph.\n",
    "\n",
    "**Input Format:**\n",
    "You will be given a list of RDF triples. Each triple is structured as `(Subject, Predicate, Object)`. All subjects, predicates, and objects are provided as human-readable string labels.\n",
    "\n",
    "**Core Instructions:**\n",
    "\n",
    "1.  **Synthesize, Don't Just List:** Do not simply state each triple as a standalone fact. Instead, analyze the connections between the triples. When the same entity appears as a subject or object in multiple triples, use this connection to form more complex and natural sentences.\n",
    "\n",
    "2.  **Accurate Verbalization:** Translate the `(Subject, Predicate, Object)` structure into grammatically correct sentences. The predicate represents the relationship between the subject and the object.\n",
    "\n",
    "3.  **Qualitative Probability:** When a \"probability\" predicate is present, you **must** convert the numerical percentage into a natural, qualitative, textual description. Do not use the raw percentage in the final text.\n",
    "\n",
    "4.  **Strict Data Adherence:** This is the most important rule. **You must not add any information that is not explicitly present in the provided set of triples.** Do not make assumptions, infer missing relationships, or use any external knowledge. The output text must be a direct and faithful verbalization of *only* the input data.\n",
    "\n",
    "5.  **Coherent Narrative:** The final output should be a single, well-formed paragraph that reads smoothly.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5632749a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_1t = \"\"\"# Prompt for Knowledge Graph Verbalization\n",
    "\n",
    "**Objective:** Your task is to convert a set of RDF triples, representing a small knowledge graph, into a coherent, natural-language paragraph.\n",
    "\n",
    "**Input Format:**\n",
    "You will be given a list of RDF triples. Each triple is structured as `(Subject, Predicate, Object)`. All subjects, predicates, and objects are provided as human-readable string labels.\n",
    "\n",
    "**Core Instructions:**\n",
    "\n",
    "1.  **Synthesize, Don't Just List:** Do not simply state each triple as a standalone fact. Instead, analyze the connections between the triples. When the same entity appears as a subject or object in multiple triples, use this connection to form more complex and natural sentences.\n",
    "\n",
    "2.  **Accurate Verbalization:** Translate the `(Subject, Predicate, Object)` structure into grammatically correct sentences. The predicate represents the relationship between the subject and the object.\n",
    "\n",
    "3.  **Qualitative Probability:** When a \"probability\" predicate is present, you **must** convert the numerical percentage into a natural, qualitative, textual description. Do not use the raw percentage in the final text.\n",
    "\n",
    "4.  **Strict Data Adherence:** This is the most important rule. **You must not add any information that is not explicitly present in the provided set of triples.** Do not make assumptions, infer missing relationships, or use any external knowledge. The output text must be a direct and faithful verbalization of *only* the input data.\n",
    "\n",
    "5.  **Coherent Narrative:** The final output should be a single, well-formed paragraph that reads smoothly.\n",
    "\n",
    "---\n",
    "\n",
    "### Example\n",
    "\n",
    "**Input RDF Triples in RDF-star:**\n",
    "<< ex:Fourier-multiplier-operator-with-symbol-that-satisfies-Hytonen-anisotropic-Mihlin-type-condition ex:is_same_as ex:Fourier-multiplier-operator-that-is-bounded-on-lp >> ex:qualifier \"=100%\" .\n",
    "**Correct Output Text:**\n",
    "\"It is certain that a Fourier multiplier operator with a symbol that satisfies the Hytonen anisotropic Mihlin-type condition is the same as a Fourier multiplier operator that is bounded on Lp.\"\n",
    "\n",
    "**Incorrect Output (Violates Rule #3 by using the numerical probability):**\n",
    "\"There is a 100% probability that a Fourier multiplier operator with a symbol that satisfies the Hytonen anisotropic Mihlin-type condition is the same as a Fourier multiplier operator that is bounded on Lp.\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9d5b9775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 68 records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 68/68 [19:25<00:00, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete. Updated data saved to 'rdf_verbal_qwen3_4b_full.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "# IMPORTANT: You must set the DASHSCOPE_API_KEY as an environment variable\n",
    "# for this script to work. For example, in your terminal:\n",
    "# export DASHSCOPE_API_KEY='your_actual_api_key'\n",
    "\n",
    "# The API key is fetched from environment variables\n",
    "api_key = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"API key not found. Please set the DASHSCOPE_API_KEY environment variable.\")\n",
    "\n",
    "# The base URL for the Dashscope service\n",
    "BASE_URL = \"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    "\n",
    "# The model you want to use\n",
    "MODEL_NAME = \"qwen3-4b\"\n",
    "# Fixed system prompt for every call\n",
    "SYSTEM_PROMPT = system_prompt_1t\n",
    "\n",
    "# User prompt template with a placeholder for the RDF-star content\n",
    "USER_PROMPT_TEMPLATE = \"\"\"Now, please verbalize the following set of RDF triples into a single, concise paragraph, following all the rules above.\n",
    "\n",
    "**Output Format:** Your response must contain **only the final verbalized paragraph** in plaintext and nothing else. Do not add any introductory phrases, comments, or explanations.\n",
    "\n",
    "**Input RDF Triples in RDF-star:**\n",
    "{}\"\"\"\n",
    "\n",
    "\n",
    "# --- File Paths ---\n",
    "INPUT_JSON_PATH = 'rdf_verbal_qwen3_235b_full.json'\n",
    "OUTPUT_JSON_PATH = 'rdf_verbal_qwen3_4b_full.json'\n",
    "\n",
    "\n",
    "def get_llm_response(client, user_prompt):\n",
    "    \"\"\"\n",
    "    Calls the LLM API using the provided client and user prompt, handling the streaming response.\n",
    "\n",
    "    Args:\n",
    "        client (OpenAI): The initialized OpenAI client.\n",
    "        user_prompt (str): The full user prompt to be sent to the LLM.\n",
    "\n",
    "    Returns:\n",
    "        str: The complete text content from the LLM's response.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=MODEL_NAME,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            extra_body={\"enable_thinking\": True},\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        answer_content = \"\"\n",
    "        for chunk in completion:\n",
    "            # Skip chunks that are for usage stats\n",
    "            if not chunk.choices:\n",
    "                continue\n",
    "            \n",
    "            delta = chunk.choices[0].delta\n",
    "            \n",
    "            if hasattr(delta, \"content\") and delta.content:\n",
    "                answer_content += delta.content\n",
    "\n",
    "        return answer_content.strip()\n",
    "\n",
    "    except Exception as e:\n",
    "        error_message = f\"Error: An exception occurred during the API call: {e}\"\n",
    "        print(error_message)\n",
    "        return error_message\n",
    "\n",
    "\n",
    "def process_records(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Loads records from the input JSON, processes each one with the LLM,\n",
    "    and saves the updated records to the output JSON.\n",
    "    \"\"\"\n",
    "    # 1. Load the input JSON file\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input file not found at '{input_path}'\")\n",
    "        return\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{input_path}'\")\n",
    "        return\n",
    "\n",
    "    dataset = data.get(\"dataset\", [])\n",
    "    if not dataset:\n",
    "        print(\"Input file does not contain a 'dataset' list or the list is empty.\")\n",
    "        return\n",
    "\n",
    "    # Initialize the LLM client once\n",
    "    client = OpenAI(\n",
    "        api_key=api_key,\n",
    "        base_url=BASE_URL,\n",
    "    )\n",
    "\n",
    "    # 2. Process each record sequentially with a progress bar\n",
    "    print(f\"Processing {len(dataset)} records...\")\n",
    "    for record in tqdm(dataset):\n",
    "        rdf_star_text = record.get(\"rdf_star\", \"\")\n",
    "        if rdf_star_text:\n",
    "            # Format the full user prompt using the template\n",
    "            user_prompt = USER_PROMPT_TEMPLATE.format(rdf_star_text)\n",
    "            \n",
    "            # Get the LLM response for the current record\n",
    "            llm_response = get_llm_response(client, user_prompt)\n",
    "            \n",
    "            # Update the record with the response\n",
    "            record[\"one_text_qwen3:4b\"] = llm_response\n",
    "\n",
    "    # 3. Save the updated data to the output JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(f\"\\nProcessing complete. Updated data saved to '{output_path}'\")\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == '__main__':\n",
    "    process_records(INPUT_JSON_PATH, OUTPUT_JSON_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "10e6d00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Processing complete!\n",
      "Updated 68 records in the file: rdf_verbal_qwen3_full.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def add_field_to_json(source_path, target_path, key_to_add='zero_text_qwen3:4b', id_key='rdf_star'):\n",
    "    \"\"\"\n",
    "    Reads a source JSON and a target JSON file. For each object in the target\n",
    "    file's dataset, it finds the corresponding object in the source dataset\n",
    "    (using a unique id_key) and adds a specified key-value pair from source to target.\n",
    "\n",
    "    Args:\n",
    "        source_path (str): The path to the JSON file to read the new data from.\n",
    "        target_path (str): The path to the JSON file that will be modified.\n",
    "        key_to_add (str): The name of the key to add to the target file.\n",
    "        id_key (str): The key to use as a unique identifier to match objects.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Step 1: Read the source file and create a lookup map for efficient access.\n",
    "        # The map will store {id_key: value_to_add}.\n",
    "        with open(source_path, 'r', encoding='utf-8') as f:\n",
    "            source_data = json.load(f)\n",
    "\n",
    "        lookup_map = {}\n",
    "        if 'dataset' in source_data and isinstance(source_data['dataset'], list):\n",
    "            for item in source_data['dataset']:\n",
    "                # Ensure both the identifier and the key to add exist in the source item\n",
    "                if id_key in item and key_to_add in item:\n",
    "                    identifier = item[id_key]\n",
    "                    value = item[key_to_add]\n",
    "                    lookup_map[identifier] = value\n",
    "        else:\n",
    "            print(f\"Warning: 'dataset' key not found or is not a list in the source file '{source_path}'.\")\n",
    "            return # Exit if source data is not as expected\n",
    "\n",
    "        # Step 2: Read the target file that we are going to modify.\n",
    "        with open(target_path, 'r', encoding='utf-8') as f:\n",
    "            target_data = json.load(f)\n",
    "\n",
    "        # Step 3: Iterate through the target data and add the new key-value pair.\n",
    "        items_modified = 0\n",
    "        if 'dataset' in target_data and isinstance(target_data['dataset'], list):\n",
    "            for item in target_data['dataset']:\n",
    "                if id_key in item:\n",
    "                    identifier = item[id_key]\n",
    "                    # If we find a matching record in our lookup map, add the new field.\n",
    "                    if identifier in lookup_map:\n",
    "                        item[key_to_add] = lookup_map[identifier]\n",
    "                        items_modified += 1\n",
    "        else:\n",
    "            print(f\"Warning: 'dataset' key not found or not a list in the target file '{target_path}'.\")\n",
    "            return\n",
    "\n",
    "        # Step 4: Write the modified data back to the target file, overwriting it.\n",
    "        with open(target_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(target_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "        print(f\"✅ Processing complete!\")\n",
    "        print(f\"Updated {items_modified} records in the file: {target_path}\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"❌ Error: The file '{e.filename}' was not found.\")\n",
    "        print(\"Please make sure the file exists and the path is correct.\")\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"❌ Error: Could not decode JSON from one of the files.\")\n",
    "        print(\"Please ensure both files contain valid JSON.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "# --- How to use this script ---\n",
    "\n",
    "# 1. Make sure this python script is in the same directory as your JSON files.\n",
    "# 2. Define the paths for your source and target files.\n",
    "#    Replace the placeholder names with your actual file names.\n",
    "\n",
    "# The file you want to get the 'zero_text_qwen3:4b' value FROM.\n",
    "source_file_path = '../rdf_verbal_qwen3-4b_full.json'\n",
    "\n",
    "# The file you want to add the 'zero_text_qwen3:4b' value TO.\n",
    "# IMPORTANT: This file will be read and then overwritten with the changes.\n",
    "target_file_path = 'rdf_verbal_qwen3_full.json'\n",
    "\n",
    "# 3. Call the function with your file paths.\n",
    "add_field_to_json(\n",
    "    source_path=source_file_path,\n",
    "    target_path=target_file_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47ba4e2-ac6d-4ad8-b8ff-0696ce1542ad",
   "metadata": {},
   "source": [
    "### Evaluate different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "456ffce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI client initialized successfully.\n",
      "Successfully read beichen_final.xlsx\n",
      "Found 168 words in column 'prob_word' and converted them to lowercase.\n",
      "Getting embeddings from OpenAI...\n",
      "Embeddings received successfully.\n",
      "Calculating similarities and finding best matches...\n",
      "Added new columns 'semantic_match' and 'similarity_score' to the data.\n",
      "Successfully wrote the results to beichen_final_with_matches.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, OpenAIError\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "# --- OpenAI Client Setup ---\n",
    "# This script uses the OpenAI Python library version 1.0.0 or later.\n",
    "# The client automatically looks for the \"OPENAI_API_KEY\" environment variable.\n",
    "# Make sure it is set in your environment before running the script.\n",
    "try:\n",
    "    client = OpenAI()\n",
    "    # A quick test to see if the client is configured with a valid key\n",
    "    client.models.list()\n",
    "    print(\"OpenAI client initialized successfully.\")\n",
    "except OpenAIError as e:\n",
    "    print(f\"Error initializing OpenAI client: {e}\")\n",
    "    print(\"Please ensure your OPENAI_API_KEY environment variable is set correctly.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Input and Output file paths for Excel files\n",
    "INPUT_FILE_PATH = 'beichen_final.xlsx'\n",
    "OUTPUT_FILE_PATH = 'beichen_final_with_matches.xlsx'\n",
    "\n",
    "# The list of words to compare against\n",
    "# FIX: Converted all words to lowercase for case-insensitive comparison.\n",
    "BINS = [\n",
    "    \"certain\", \"almost certain\", \"highly likely\", \"very good chance\",\n",
    "    \"we believe\", \"likely\", \"probable\", \"probably\", \"better than even\",\n",
    "    \"about even\", \"probably not\", \"we doubt\", \"unlikely\", \"improbable\",\n",
    "    \"chances are slight\", \"little chance\", \"highly unlikely\",\n",
    "    \"almost no chance\", \"impossible\"\n",
    "]\n",
    "\n",
    "# The column in your Excel file that contains the words to be analyzed\n",
    "COLUMN_NAME = \"prob_word\"\n",
    "\n",
    "# --- Functions ---\n",
    "\n",
    "def get_embeddings(texts, model=\"text-embedding-3-large\"):\n",
    "    \"\"\"\n",
    "    Gets embeddings for a list of texts using the OpenAI API.\n",
    "    This function is updated for openai library v1.0.0+.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Use the global client object's 'embeddings.create' method\n",
    "        response = client.embeddings.create(input=texts, model=model)\n",
    "        # The response from the v1.0.0+ library is an object, not a dictionary.\n",
    "        # Access its attributes using dot notation (e.g., response.data).\n",
    "        # Each item in response.data is an Embedding object with an 'embedding' attribute.\n",
    "        embeddings = [item.embedding for item in response.data]\n",
    "        return embeddings\n",
    "    except OpenAIError as e:\n",
    "        print(f\"An error occurred with the OpenAI API: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_best_match(prob_word_embeddings, bin_embeddings, bins_list):\n",
    "    \"\"\"\n",
    "    Finds the best matching bin for each probability word based on cosine similarity.\n",
    "    \"\"\"\n",
    "    if not prob_word_embeddings or not bin_embeddings:\n",
    "        return [], []\n",
    "\n",
    "    # Calculate cosine similarity between all prob_words and all bins\n",
    "    similarity_matrix = cosine_similarity(prob_word_embeddings, bin_embeddings)\n",
    "\n",
    "    # Find the index of the bin with the highest similarity for each prob_word\n",
    "    best_match_indices = np.argmax(similarity_matrix, axis=1)\n",
    "\n",
    "    # Get the best matching words and their corresponding scores\n",
    "    best_matches = [bins_list[i] for i in best_match_indices]\n",
    "    best_scores = [similarity_matrix[i, j] for i, j in enumerate(best_match_indices)]\n",
    "\n",
    "    return best_matches, best_scores\n",
    "\n",
    "def process_file(input_path, output_path, column_name, bins_list):\n",
    "    \"\"\"\n",
    "    Main function to read the file, process the data, and write the output.\n",
    "    \"\"\"\n",
    "    # 1. Read the input Excel file\n",
    "    try:\n",
    "        # Use read_excel for .xlsx files\n",
    "        df = pd.read_excel(input_path)\n",
    "        print(f\"Successfully read {input_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_path}' was not found.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Excel file: {e}\")\n",
    "        print(\"Please ensure 'openpyxl' is installed: pip install openpyxl\")\n",
    "        return\n",
    "\n",
    "    # 2. Get the list of words from the specified column\n",
    "    # FIX: Ensure all items are strings and convert to lowercase to avoid case sensitivity issues.\n",
    "    prob_words = df[column_name].astype(str).str.lower().tolist()\n",
    "    print(f\"Found {len(prob_words)} words in column '{column_name}' and converted them to lowercase.\")\n",
    "\n",
    "    # 3. Get embeddings for both lists of words\n",
    "    print(\"Getting embeddings from OpenAI...\")\n",
    "    prob_word_embeddings = get_embeddings(prob_words)\n",
    "    bin_embeddings = get_embeddings(bins_list)\n",
    "\n",
    "    if not prob_word_embeddings or not bin_embeddings:\n",
    "        print(\"Could not retrieve embeddings. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"Embeddings received successfully.\")\n",
    "\n",
    "    # 4. Find the best match for each word\n",
    "    print(\"Calculating similarities and finding best matches...\")\n",
    "    best_matches, best_scores = find_best_match(prob_word_embeddings, bin_embeddings, bins_list)\n",
    "\n",
    "    if not best_matches:\n",
    "        print(\"Could not find best matches. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # 5. Add the results to the DataFrame\n",
    "    df['semantic_match'] = best_matches\n",
    "    df['similarity_score'] = best_scores\n",
    "    print(\"Added new columns 'semantic_match' and 'similarity_score' to the data.\")\n",
    "\n",
    "    # 6. Write the updated DataFrame to a new Excel file\n",
    "    try:\n",
    "        # Use to_excel for .xlsx files\n",
    "        df.to_excel(output_path, index=False)\n",
    "        print(f\"Successfully wrote the results to {output_path}\")\n",
    "    except IOError:\n",
    "        print(f\"Error: Could not write to the file '{output_path}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the Excel file: {e}\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    process_file(INPUT_FILE_PATH, OUTPUT_FILE_PATH, COLUMN_NAME, BINS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
